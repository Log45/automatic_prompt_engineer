{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Prompts with **Automatic Prompt Engineer** (APE)\n",
    "\n",
    "This notebook demonstrates how to use Automatic Prompt Engineer (APE) (arxiv link) to optimize prompts for text generation. In its simplest form, APE takes as input a dataset (a list of inputs and a list of outputs), a prompt template, and optimizes this prompt template so that it generates the outputs given the inputs.\n",
    "\n",
    "APE accomplishes this in two steps. First, it uses a language model to generate a set of candidate prompts. Then, it uses a prompt evaluation function to evaluate the quality of each candidate prompt. Finally, it returns the prompt with the highest evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a simple dataset consisting of words and their antonyms.\n",
    "words = [\"sane\", \"direct\", \"informally\", \"unpopular\", \"subtractive\", \"nonresidential\",\n",
    "    \"inexact\", \"uptown\", \"incomparable\", \"powerful\", \"gaseous\", \"evenly\", \"formality\",\n",
    "    \"deliberately\", \"off\"]\n",
    "antonyms = [\"insane\", \"indirect\", \"formally\", \"popular\", \"additive\", \"residential\",\n",
    "    \"exact\", \"downtown\", \"comparable\", \"powerless\", \"solid\", \"unevenly\", \"informality\",\n",
    "    \"accidentally\", \"on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to define the format of the prompt that we are using.\n",
    "\n",
    "eval_template = \\\n",
    "\"\"\"Instruction: [PROMPT]\n",
    "Input: [INPUT]\n",
    "Output: [OUTPUT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config before update: {'generation': {'num_subsamples': 5, 'num_demos': 5, 'num_prompts_per_subsample': 10, 'model': {'name': 'OPT', 'batch_size': 100, 'gpt_config': {'model': 'facebook/opt-2.7b', 'temperature': 0.9, 'max_tokens': 50, 'top_p': 0.9, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}, 'evaluation': {'method': 'bandits', 'rounds': 20, 'num_prompts_per_round': 0.334, 'bandit_method': 'ucb', 'bandit_config': {'c': 1.0}, 'base_eval_method': 'likelihood', 'base_eval_config': {'num_samples': 5, 'num_few_shot': 5, 'model': {'name': 'GPT_forward', 'batch_size': 250, 'gpt_config': {'model': 'facebook/opt-2.7b', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}}, 'demo': {'model': {'name': 'GPT_forward', 'batch_size': 500, 'gpt_config': {'model': 'text-davinci-002', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}}\n",
      "Config after update: {'generation': {'num_subsamples': 5, 'num_demos': 5, 'num_prompts_per_subsample': 10, 'model': {'name': 'OPT', 'batch_size': 100, 'gpt_config': {'model': 'facebook/opt-2.7b', 'temperature': 0.9, 'max_tokens': 50, 'top_p': 0.9, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}, 'evaluation': {'method': 'bandits', 'num_samples': 50, 'num_few_shot': 5, 'model': {'name': 'GPT_forward', 'batch_size': 500, 'gpt_config': {'model': 'text-davinci-002', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}, 'rounds': 20, 'num_prompts_per_round': 0.334, 'bandit_method': 'ucb', 'bandit_config': {'c': 1.0}, 'base_eval_method': 'likelihood', 'base_eval_config': {'num_samples': 5, 'num_few_shot': 5, 'model': {'name': 'GPT_forward', 'batch_size': 250, 'gpt_config': {'model': 'facebook/opt-2.7b', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}}, 'demo': {'model': {'name': 'GPT_forward', 'batch_size': 500, 'gpt_config': {'model': 'text-davinci-002', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}}}\n",
      "Generating prompts...\n",
      "[OPT] Generating 50 completions, split into 1 batches of size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object OPT.__generate_text.<locals>.<genexpr> at 0x7f81db359f20>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [12:24<00:00, 744.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned 10 prompts. Deduplicating...\n",
      "Model: {'name': 'GPT_forward', 'batch_size': 500, 'gpt_config': {'model': 'text-davinci-002', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}\n",
      "Deduplicated to 10 prompts.\n",
      "Evaluating prompts...\n",
      "Model: {'name': 'OPT', 'batch_size': 500, 'gpt_config': {'model': 'facebook/opt-2.7b', 'temperature': 0.7, 'max_tokens': 200, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating prompts:   0%|                                                                        | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Now, let's use APE to find prompts that generate antonyms for each word.\n",
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "result, demo_fn = ape.simple_ape(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    "    eval_model=\"facebook/opt-2.7b\",\n",
    "    prompt_gen_model=\"facebook/opt-2.7b\",\n",
    "    prompt_gen_mode=\"opt\",\n",
    "    prompt_gen_batch_size=100,\n",
    "    eval_batch_size=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with a prompt written by a human:\n",
    "\n",
    "\"*Write an antonym to the following word.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "manual_prompt = \"Write an antonym to the following word.\"\n",
    "\n",
    "human_result = ape.simple_eval(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    "    prompts=[manual_prompt],\n",
    "    eval_model=\"facebook/opt-2.7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(human_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2afcb7a2e6fcb9490d448e607abf9226c3f7acca28baeea9bc24b456562037f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
